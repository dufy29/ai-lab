# ai-lab
深度学习研习，包括训练，部署等

## 部署


- [mmdeploy模型部署系列](https://github.com/open-mmlab/mmdeploy/tree/master/docs/zh_cn/tutorial)
- [知乎：模型部署那些事](https://www.zhihu.com/column/c_1497987564452114432)


## llm微调

- [Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
- 论文《 LoRA: Low-Rank Adaptation of Large Language Models》
- huggingface/peft, https://github.com/huggingface/peft

    Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. 
    
    Supported methods:
    - LoRA
    - Prefix Tuning
    - P-Tuning
    - Prompt Tuning
    - AdaLoRA